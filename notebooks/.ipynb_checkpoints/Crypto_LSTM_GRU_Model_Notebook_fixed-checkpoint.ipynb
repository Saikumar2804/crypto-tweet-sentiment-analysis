{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Step 1: Importing Required Libraries\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Basic libraries for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Natural Language Processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# Machine Learning and Deep Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Dropout, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 2: Load the Dataset\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Load the dataset (assuming 'Bitcoin_tweets.csv' is available in the working directory)\ndf = pd.read_csv('/path/to/Bitcoin_tweets.csv')\nprint(df.head())\n\n# Check for missing values\nprint(df.isnull().sum())\n\n# Drop rows with missing values if any\ndf.dropna(inplace=True)\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 3: Data Preprocessing (Tokenization, Lemmatization, Stop Words Removal)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Initialize preprocessing tools\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\n# Function to preprocess the text\ndef preprocess_text(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stopwords and lemmatize\n    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n    \n    return ' '.join(filtered_tokens)\n\n# Apply the preprocessing to the tweet column\ndf['cleaned_text'] = df['tweet'].apply(preprocess_text)\n\nprint(df['cleaned_text'].head())\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 4: Tokenization and Padding\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Tokenize the words and convert them to sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(df['cleaned_text'])\n\nX = tokenizer.texts_to_sequences(df['cleaned_text'])\nX = pad_sequences(X, maxlen=100)  # Padding sequences to have the same length\n\n# Encode the target variable (emotion or sentiment column)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['dominant_emotion'])\n\n# Convert labels to categorical (one-hot encoding)\ny = pd.get_dummies(df['dominant_emotion']).values\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 5: Train-Test Split\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 6: Build the LSTM-GRU Hybrid Model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Build the LSTM-GRU model\nmodel = Sequential()\n\n# Embedding layer\nmodel.add(Embedding(input_dim=5000, output_dim=200, input_length=100))\n\n# LSTM layer\nmodel.add(LSTM(units=128, return_sequences=True))\nmodel.add(Dropout(0.5))\n\n# GRU layer\nmodel.add(GRU(units=64))\n\n# Fully connected layer\nmodel.add(Dense(16, activation='relu'))\n\n# Output layer (using softmax for multi-class classification)\nmodel.add(Dense(y.shape[1], activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nprint(model.summary())\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 7: Model Training\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 8: Evaluate the Model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy:.4f}')\n\n# Plot accuracy and loss curves\nplt.figure(figsize=(12, 6))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.title('Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title('Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 9: Save the Model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Save the trained model\nmodel.save('lstm_gru_cryptocurrency_tweet_model.h5')\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 10: Predict and Visualize Results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# Make predictions on the test set\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_true, y_pred_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n\n# Classification Report\nprint(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}