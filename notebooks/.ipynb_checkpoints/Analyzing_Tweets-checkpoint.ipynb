{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a11e13-ffa1-421a-8038-2ef4a7ee0f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id       date                                               text  \\\n",
      "0      2 2018-03-23  RT @tippereconomy: Another use case for #block...   \n",
      "1   8167 2018-03-23  @akbar_ohi @YoustockProject This article expla...   \n",
      "2  38675 2018-03-23  RT @Rubingh: In the world of hype, #blockchain...   \n",
      "3  28785 2018-03-23  Name: INS Ecosystem\\nSymbol: INS\\n24 hour chan...   \n",
      "4   8745 2018-03-23  Crypto Collectibles Are Worthless Without a We...   \n",
      "\n",
      "       Screen_name                                        Source  \\\n",
      "0     hojachotopur  [u'blockchain', u'Tipper', u'TipperEconomy']   \n",
      "1      JapaMahatma                                            []   \n",
      "2        racrozier                               [u'blockchain']   \n",
      "3  moneyblockchain                                            []   \n",
      "4   CoinbeagleNews                                            []   \n",
      "\n",
      "                                                Link     Sentiment  \\\n",
      "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  ['positive']   \n",
      "1  <a href=\"http://twitter.com/download/iphone\" r...  ['negative']   \n",
      "2  <a href=\"http://twitter.com/download/iphone\" r...  ['negative']   \n",
      "3  <a href=\"http://www.cryptocleo.com\" rel=\"nofol...  ['negative']   \n",
      "4  <a href=\"https://www.cointaco.com\" rel=\"nofoll...  ['negative']   \n",
      "\n",
      "   sent_score  New_Sentiment_Score  New_Sentiment_State  BERT Labels  \\\n",
      "0           1             0.136364                    1            0   \n",
      "1          -1            -0.187500                   -1            0   \n",
      "2          -1            -0.065833                   -1            0   \n",
      "3          -1            -0.400000                   -1            0   \n",
      "4          -1            -0.800000                   -1            0   \n",
      "\n",
      "                                        cleaned_text cryptocurrency    price  \\\n",
      "0  RT  Another use case for blockchain and Tipper...       Ethereum  7794.63   \n",
      "1  This article explains a little about what Im d...       Ethereum  7794.63   \n",
      "2  RT  In the world of hype blockchain ranks high...       Ethereum  7794.63   \n",
      "3  Name INS Ecosystem\\nSymbol INS\\n24 hour change...       Ethereum  7794.63   \n",
      "4  Crypto Collectibles Are Worthless Without a We...       Ethereum  7794.63   \n",
      "\n",
      "       trade  \n",
      "0  814874.11  \n",
      "1  814874.11  \n",
      "2  814874.11  \n",
      "3  814874.11  \n",
      "4  814874.11  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "tweets_df = pd.read_csv('C:/Users/HP/crypto-tweet-analysis/data/Bitcoin_tweets.csv')\n",
    "market_data = pd.read_csv('C:/Users/HP/crypto-tweet-analysis/data/modified_cryptocurrency_prices.csv')\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "market_data['date'] = pd.to_datetime(market_data['date'])\n",
    "\n",
    "# Sort both dataframes by 'date' before using merge_asof\n",
    "tweets_df = tweets_df.sort_values('date')\n",
    "market_data = market_data.sort_values('date')\n",
    "\n",
    "# Merge the datasets based on the 'date'\n",
    "# You may need to round or align them to a common time frame if necessary\n",
    "merged_data = pd.merge_asof(tweets_df, market_data, on='date')\n",
    "\n",
    "# Display the first few rows of the merged data\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a0440b-07ab-4e06-abc8-5d9246480f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      RT @tippereconomy: Another use case for #block...   \n",
      "45195  @akbar_ohi @YoustockProject This article expla...   \n",
      "45196  RT @Rubingh: In the world of hype, #blockchain...   \n",
      "45197  Name: INS Ecosystem\\nSymbol: INS\\n24 hour chan...   \n",
      "45198  Crypto Collectibles Are Worthless Without a We...   \n",
      "\n",
      "                                           cleaned_tweet  \n",
      "0      rt another use case for and the can unseat fac...  \n",
      "45195  this article explains a little about what im d...  \n",
      "45196  rt in the world of hype ranks highly partly be...  \n",
      "45197  name ins ecosystem symbol ins hour change pric...  \n",
      "45198  crypto collectibles are worthless without a we...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the tweet text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters (punctuation, numbers, etc.)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the 'Tweet' column\n",
    "tweets_df['cleaned_tweet'] = tweets_df['text'].apply(clean_text)\n",
    "\n",
    "# View the cleaned data\n",
    "print(tweets_df[['text', 'cleaned_tweet']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2be8f3a-fb41-4da8-8d3b-8a66cbd9b068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           cleaned_tweet  \\\n",
      "0      rt another use case for and the can unseat fac...   \n",
      "45195  this article explains a little about what im d...   \n",
      "45196  rt in the world of hype ranks highly partly be...   \n",
      "45197  name ins ecosystem symbol ins hour change pric...   \n",
      "45198  crypto collectibles are worthless without a we...   \n",
      "\n",
      "                                         tokenized_tweet  \n",
      "0      [rt, another, use, case, for, and, the, can, u...  \n",
      "45195  [this, article, explains, a, little, about, wh...  \n",
      "45196  [rt, in, the, world, of, hype, ranks, highly, ...  \n",
      "45197  [name, ins, ecosystem, symbol, ins, hour, chan...  \n",
      "45198  [crypto, collectibles, are, worthless, without...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Tokenize the cleaned tweets\n",
    "tweets_df['tokenized_tweet'] = tweets_df['cleaned_tweet'].apply(word_tokenize)\n",
    "\n",
    "# View the tokenized data\n",
    "print(tweets_df[['cleaned_tweet', 'tokenized_tweet']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5234dab2-eb4c-4efa-bccb-f3fb3dbb43ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         tokenized_tweet  \\\n",
      "0      [rt, another, use, case, for, and, the, can, u...   \n",
      "45195  [this, article, explains, a, little, about, wh...   \n",
      "45196  [rt, in, the, world, of, hype, ranks, highly, ...   \n",
      "45197  [name, ins, ecosystem, symbol, ins, hour, chan...   \n",
      "45198  [crypto, collectibles, are, worthless, without...   \n",
      "\n",
      "                                        lemmatized_tweet  \n",
      "0      [rt, another, use, case, for, and, the, can, u...  \n",
      "45195  [this, article, explains, a, little, about, wh...  \n",
      "45196  [rt, in, the, world, of, hype, rank, highly, p...  \n",
      "45197  [name, in, ecosystem, symbol, in, hour, change...  \n",
      "45198  [crypto, collectible, are, worthless, without,...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization to the tokenized tweets\n",
    "tweets_df['lemmatized_tweet'] = tweets_df['tokenized_tweet'].apply(lemmatize_tokens)\n",
    "\n",
    "# View the lemmatized data\n",
    "print(tweets_df[['tokenized_tweet', 'lemmatized_tweet']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86c4c62-b20d-4f67-a7e4-6d89945835d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        lemmatized_tweet  \\\n",
      "0      [rt, another, use, case, for, and, the, can, u...   \n",
      "45195  [this, article, explains, a, little, about, wh...   \n",
      "45196  [rt, in, the, world, of, hype, rank, highly, p...   \n",
      "45197  [name, in, ecosystem, symbol, in, hour, change...   \n",
      "45198  [crypto, collectible, are, worthless, without,...   \n",
      "\n",
      "                                            final_tokens  \n",
      "0      [rt, another, use, case, unseat, facebook, cha...  \n",
      "45195                    [article, explains, little, im]  \n",
      "45196  [rt, world, hype, rank, highly, partly, hard, ...  \n",
      "45197  [name, ecosystem, symbol, hour, change, price,...  \n",
      "45198  [crypto, collectible, worthless, without, webs...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply stop word removal\n",
    "tweets_df['final_tokens'] = tweets_df['lemmatized_tweet'].apply(remove_stopwords)\n",
    "\n",
    "# View the processed data\n",
    "print(tweets_df[['lemmatized_tweet', 'final_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9b2a6b-7fcd-4c2c-8ddc-c6a11b48d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1  180   88  147  842  472    8  407   74   48    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  45  288  199  121    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   1   25 1836    7 1383 4230  209 4230 2780 3532    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  11  750   10    4    8    3    7    6    5   14    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  15  149  151  197  164    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)  # Set max vocabulary size\n",
    "tokenizer.fit_on_texts(tweets_df['final_tokens'])\n",
    "\n",
    "# Convert tokens to sequences\n",
    "tweets_df['sequences'] = tokenizer.texts_to_sequences(tweets_df['final_tokens'])\n",
    "\n",
    "# Pad sequences to ensure uniform length (padding at the end)\n",
    "X = pad_sequences(tweets_df['sequences'], maxlen=100, padding='post')\n",
    "\n",
    "# View the final sequences\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17dc5658-aa1c-41f5-b13f-97ac9ffe785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           cleaned_tweet  sentiment  \\\n",
      "0      rt another use case for and the can unseat fac...   0.136364   \n",
      "45195  this article explains a little about what im d...  -0.187500   \n",
      "45196  rt in the world of hype ranks highly partly be...  -0.065833   \n",
      "45197  name ins ecosystem symbol ins hour change pric...  -0.400000   \n",
      "45198  crypto collectibles are worthless without a we...  -0.800000   \n",
      "\n",
      "      sentiment_label  \n",
      "0            Positive  \n",
      "45195        Negative  \n",
      "45196        Negative  \n",
      "45197        Negative  \n",
      "45198        Negative  \n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to get the sentiment polarity of a tweet\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Returns polarity between -1 and 1\n",
    "\n",
    "# Apply the sentiment analysis function to the cleaned tweet column\n",
    "tweets_df['sentiment'] = tweets_df['cleaned_tweet'].apply(get_sentiment)\n",
    "\n",
    "# Label tweets as Positive, Negative, or Neutral based on polarity score\n",
    "def label_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "tweets_df['sentiment_label'] = tweets_df['sentiment'].apply(label_sentiment)\n",
    "\n",
    "# View the data with sentiment labels\n",
    "print(tweets_df[['cleaned_tweet', 'sentiment', 'sentiment_label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cebac7-e852-4010-bcf2-c01de44039b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           cleaned_tweet  subjectivity\n",
      "0      rt another use case for and the can unseat fac...      0.500000\n",
      "45195  this article explains a little about what im d...      0.500000\n",
      "45196  rt in the world of hype ranks highly partly be...      0.540833\n",
      "45197  name ins ecosystem symbol ins hour change pric...      0.825000\n",
      "45198  crypto collectibles are worthless without a we...      0.900000\n"
     ]
    }
   ],
   "source": [
    "# Function to get subjectivity\n",
    "def get_subjectivity(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.subjectivity  # Returns subjectivity between 0 (objective) and 1 (subjective)\n",
    "\n",
    "# Apply subjectivity analysis\n",
    "tweets_df['subjectivity'] = tweets_df['cleaned_tweet'].apply(get_subjectivity)\n",
    "\n",
    "# View subjectivity\n",
    "print(tweets_df[['cleaned_tweet', 'subjectivity']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ffbf5-ca18-4083-b2f7-72ebe35e6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained emotion detection model using Hugging Face\n",
    "emotion_classifier = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base', return_all_scores=True)\n",
    "\n",
    "# Define batch size for processing\n",
    "batch_size = 5000  # Adjust the batch size based on your system's capabilities\n",
    "num_batches = len(tweets_df) // batch_size + 1\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "all_emotion_scores = []\n",
    "\n",
    "# Process the data in batches\n",
    "for i in range(num_batches):\n",
    "    batch_texts = tweets_df['cleaned_tweet'].iloc[i*batch_size : (i+1)*batch_size].tolist()\n",
    "    \n",
    "    # Apply emotion detection on the batch\n",
    "    batch_emotions = emotion_classifier(batch_texts)\n",
    "    \n",
    "    # Append the results for the batch\n",
    "    all_emotion_scores.extend(batch_emotions)\n",
    "\n",
    "# Add the emotion scores to the DataFrame\n",
    "tweets_df['emotion_scores'] = all_emotion_scores\n",
    "\n",
    "# Function to extract key emotions\n",
    "def extract_key_emotions(scores):\n",
    "    emotions = {}\n",
    "    for score in scores:\n",
    "        emotions[score['label']] = score['score']\n",
    "    return emotions\n",
    "\n",
    "# Apply the function to get key emotions for each tweet\n",
    "tweets_df['key_emotions'] = tweets_df['emotion_scores'].apply(extract_key_emotions)\n",
    "\n",
    "# View the tweet data with emotion scores\n",
    "print(tweets_df[['cleaned_tweet', 'key_emotions']].head())\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "tweets_df.to_csv('tweets_with_emotions_optimized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b2ed5-09b9-486d-aadd-b95e8542a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained emotion detection model using Hugging Face\n",
    "emotion_classifier = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base', return_all_scores=True)\n",
    "\n",
    "# Function to get emotion scores\n",
    "def get_emotions(text):\n",
    "    scores = emotion_classifier(text)\n",
    "    return scores[0]  # Return scores for all emotions\n",
    "\n",
    "# Apply emotion detection\n",
    "tweets_df['emotion_scores'] = tweets_df['cleaned_tweet'].apply(get_emotions)\n",
    "\n",
    "# Extract key emotions\n",
    "def extract_key_emotions(scores):\n",
    "    emotions = {}\n",
    "    for score in scores:\n",
    "        emotions[score['label']] = score['score']\n",
    "    return emotions\n",
    "\n",
    "# Apply the function to get key emotions\n",
    "tweets_df['key_emotions'] = tweets_df['emotion_scores'].apply(extract_key_emotions)\n",
    "\n",
    "# View the data with emotion scores\n",
    "print(tweets_df[['cleaned_tweet', 'key_emotions']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b25fe8fd-46d8-4bfc-b97e-a8029a4f239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           cleaned_tweet dominant_emotion\n",
      "25808  rt big congratulations to the organisers of th...              joy\n",
      "22468  rt we already have an ios prototype of the bet...          neutral\n",
      "17938  rt cryptonsleeps icx krw trading market just w...          neutral\n",
      "15084  rt hi everybody we want to remind you that onl...          neutral\n",
      "23325                       rt join the gym rewards beta          neutral\n"
     ]
    }
   ],
   "source": [
    "# Function to extract the most prominent emotion\n",
    "def get_top_emotion(emotions):\n",
    "    return max(emotions, key=emotions.get)\n",
    "\n",
    "# Apply the function to extract the dominant emotion for each tweet\n",
    "tweets_df['dominant_emotion'] = tweets_df['key_emotions'].apply(get_top_emotion)\n",
    "\n",
    "# View the tweet data with dominant emotions\n",
    "print(tweets_df[['cleaned_tweet', 'dominant_emotion']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27c3f5b9-f7b0-45ea-be84-793162920f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
    "\n",
    "# Assuming max_len is the length of the input sequences and vocab_size is the size of the vocabulary\n",
    "max_len = 100  # Example input sequence length\n",
    "vocab_size = 5000  # Example vocabulary size\n",
    "embedding_dim = 128  # Size of the embedding vectors\n",
    "\n",
    "# Define the LSTM-GRU model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer (input_dim should match the vocabulary size, output_dim is the embedding size)\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# Add an LSTM layer with 128 units\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "\n",
    "# Add a Dropout layer for regularization\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a GRU layer with 64 units\n",
    "model.add(GRU(units=64))\n",
    "\n",
    "# Add a Dropout layer for regularization\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a Dense layer with softmax activation (for classification)\n",
    "model.add(Dense(units=3, activation='softmax'))  # Assuming 3 classes for sentiment (positive, neutral, negative)\n",
    "\n",
    "# Compile the model using Adam optimizer and categorical crossentropy loss (for multi-class classification)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663657c3-9dc2-49ee-9286-8279efcdfb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
